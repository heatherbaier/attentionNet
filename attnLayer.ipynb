{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('qnn': conda)",
   "metadata": {
    "interpreter": {
     "hash": "4978dc1864a661fcd348fb688fbace178c7388184fc0914a9258717a7a8394e1"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "class channelPool(torch.nn.Module):\n",
    "    def __init__(self, in_channels, h, w, batch_size):\n",
    "        super(channelPool, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.h = h\n",
    "        self.w = h\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # torch.reshape(v, (1,2,2))\n",
    "        x, i = torch.max(x, dim = 1)\n",
    "        return torch.reshape(x, (self.batch_size, 1, self.h, self.w))\n",
    "\n",
    "\n",
    "class spatialPool(torch.nn.Module):\n",
    "    def __init__(self, in_channels, batch_size):\n",
    "        super(spatialPool, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, i = torch.max(x, dim = -1)\n",
    "        x, i = torch.max(x, dim = -1)\n",
    "        return torch.reshape(x, (self.batch_size, self.in_channels, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "metadata": {},
     "execution_count": 291
    }
   ],
   "source": [
    "32/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_CHANNELS = 32\n",
    "H = 224\n",
    "W = 224\n",
    "batch_size = 2\n",
    "\n",
    "input = torch.tensor(np.random.random((batch_size, IN_CHANNELS, H, W)), dtype = torch.float32)\n",
    "# input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sp = spatialPool(in_channels = IN_CHANNELS)\n",
    "# v = sp(input)\n",
    "# # v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cp = channelPool(in_channels = IN_CHANNELS)\n",
    "# v = cp(input)\n",
    "# v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "class attnNet(torch.nn.Module):\n",
    "    def __init__(self, in_channels, h, w, batch_size):\n",
    "        super(attnNet, self).__init__()\n",
    "        self.sP = spatialPool(in_channels = in_channels, batch_size = batch_size)\n",
    "        self.cP = channelPool(in_channels = in_channels, h = h, w = w, batch_size = batch_size)\n",
    "        self.out_channels = int(in_channels/16)\n",
    "        self.convR = torch.nn.Conv2d(in_channels = in_channels, out_channels = self.out_channels, kernel_size = (1,1), bias=True)\n",
    "        self.convA = torch.nn.Conv2d(in_channels = self.out_channels, out_channels = self.out_channels, kernel_size = (1,1), bias=True)\n",
    "        self.convB = torch.nn.Conv2d(in_channels = self.out_channels, out_channels = self.out_channels, kernel_size = (3,3), bias=True, padding = 1)\n",
    "        self.convC = torch.nn.Conv2d(in_channels = self.out_channels, out_channels = self.out_channels, kernel_size = (7,7), bias=True, padding = 3)\n",
    "        self.convE = torch.nn.Conv2d(in_channels = self.out_channels * 3, out_channels = in_channels, kernel_size = (1,1), bias=True)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        For a given input feature mapFin∈RC×H×W, the global pooling  operations along the spatial and channel dimensions are applied respectively to obtain the channel descriptorFc∈RC×1×1and the spatial descriptorFs∈R1×H×W\n",
    "        '''\n",
    "        fs = self.sP(x)\n",
    "        fc = self.cP(x)\n",
    "        '''\n",
    "        Then, theFcandFs are expanded to the size ofC×H×W. Element-wise multiplication(Eq.9) is used for computing the initial integrated 3D spatial-channel descriptorFsc∈RC×H×W. Fsc¼Fs⊗Fc\n",
    "        '''\n",
    "        fsc = torch.mul(fs, fc)\n",
    "        '''\n",
    "        Then, a convolution block is applied toFscto refine the spatial-channel dependencies.\n",
    "        '''\n",
    "        '''\n",
    "        A convolution with 1 × 1 kernel and a channel reduction ratior= 16 is applied in the first subblock for reducingcomputational burden and the number of parameters. \n",
    "        '''\n",
    "        r = self.convR(fsc)\n",
    "        '''The middle subblock contains three convolution operations with the kernelsizes of 1 × 1, 3 × 3, and 7 × 7 separately. Hence, the effective usageof the contextual information can be guaranteed by the different receptive field\n",
    "        '''\n",
    "        a = self.convA(r)\n",
    "        b = self.convB(r)\n",
    "        c = self.convC(r)\n",
    "        '''\n",
    "        The outputs of the three convolutions are concatenated...\n",
    "        '''\n",
    "        cat = torch.cat((a,b,c), dim = 1)\n",
    "        '''\n",
    "        ...and fed into the third subblock with a kernel size of 1 × 1 to output a feature map with the same channel number as the input map. \n",
    "        '''\n",
    "        e = self.convE(cat)\n",
    "        '''Finally, a sigmoid activationσis applied to compute the final3D spatial-channel attention mapMsc(Fin)∈RC×H×W, i.e.'''\n",
    "        out = self.sigmoid(e)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[[0.4665, 0.4559, 0.4573,  ..., 0.4568, 0.4594, 0.4650],\n",
       "          [0.4688, 0.4553, 0.4590,  ..., 0.4576, 0.4589, 0.4664],\n",
       "          [0.4651, 0.4520, 0.4546,  ..., 0.4555, 0.4558, 0.4640],\n",
       "          ...,\n",
       "          [0.4645, 0.4516, 0.4506,  ..., 0.4525, 0.4520, 0.4596],\n",
       "          [0.4656, 0.4518, 0.4520,  ..., 0.4537, 0.4517, 0.4596],\n",
       "          [0.4667, 0.4602, 0.4600,  ..., 0.4646, 0.4613, 0.4679]],\n",
       "\n",
       "         [[0.4248, 0.4277, 0.4279,  ..., 0.4207, 0.4159, 0.4089],\n",
       "          [0.4231, 0.4267, 0.4257,  ..., 0.4166, 0.4124, 0.4063],\n",
       "          [0.4241, 0.4266, 0.4269,  ..., 0.4182, 0.4128, 0.4067],\n",
       "          ...,\n",
       "          [0.4207, 0.4258, 0.4273,  ..., 0.4173, 0.4128, 0.4096],\n",
       "          [0.4185, 0.4220, 0.4226,  ..., 0.4153, 0.4112, 0.4087],\n",
       "          [0.4141, 0.4152, 0.4149,  ..., 0.4076, 0.4050, 0.4071]],\n",
       "\n",
       "         [[0.4301, 0.4261, 0.4274,  ..., 0.4272, 0.4283, 0.4269],\n",
       "          [0.4282, 0.4241, 0.4256,  ..., 0.4253, 0.4264, 0.4268],\n",
       "          [0.4262, 0.4222, 0.4233,  ..., 0.4241, 0.4245, 0.4256],\n",
       "          ...,\n",
       "          [0.4264, 0.4216, 0.4207,  ..., 0.4221, 0.4221, 0.4223],\n",
       "          [0.4269, 0.4218, 0.4216,  ..., 0.4224, 0.4222, 0.4227],\n",
       "          [0.4252, 0.4192, 0.4190,  ..., 0.4222, 0.4206, 0.4225]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.4991, 0.5041, 0.5042,  ..., 0.5042, 0.5051, 0.5070],\n",
       "          [0.5003, 0.5065, 0.5040,  ..., 0.5078, 0.5079, 0.5080],\n",
       "          [0.5012, 0.5071, 0.5049,  ..., 0.5071, 0.5082, 0.5088],\n",
       "          ...,\n",
       "          [0.5026, 0.5053, 0.5050,  ..., 0.5072, 0.5085, 0.5079],\n",
       "          [0.5024, 0.5068, 0.5060,  ..., 0.5083, 0.5100, 0.5084],\n",
       "          [0.5054, 0.5081, 0.5089,  ..., 0.5099, 0.5115, 0.5070]],\n",
       "\n",
       "         [[0.4599, 0.4471, 0.4488,  ..., 0.4515, 0.4555, 0.4605],\n",
       "          [0.4601, 0.4448, 0.4494,  ..., 0.4509, 0.4542, 0.4618],\n",
       "          [0.4559, 0.4416, 0.4444,  ..., 0.4485, 0.4510, 0.4591],\n",
       "          ...,\n",
       "          [0.4567, 0.4419, 0.4402,  ..., 0.4458, 0.4470, 0.4535],\n",
       "          [0.4587, 0.4433, 0.4433,  ..., 0.4472, 0.4471, 0.4540],\n",
       "          [0.4587, 0.4491, 0.4487,  ..., 0.4563, 0.4540, 0.4602]],\n",
       "\n",
       "         [[0.4208, 0.4243, 0.4231,  ..., 0.4242, 0.4237, 0.4228],\n",
       "          [0.4217, 0.4254, 0.4250,  ..., 0.4252, 0.4248, 0.4236],\n",
       "          [0.4226, 0.4259, 0.4263,  ..., 0.4258, 0.4258, 0.4241],\n",
       "          ...,\n",
       "          [0.4223, 0.4274, 0.4278,  ..., 0.4272, 0.4273, 0.4261],\n",
       "          [0.4222, 0.4269, 0.4273,  ..., 0.4266, 0.4266, 0.4256],\n",
       "          [0.4233, 0.4271, 0.4271,  ..., 0.4252, 0.4259, 0.4244]]],\n",
       "\n",
       "\n",
       "        [[[0.4662, 0.4562, 0.4576,  ..., 0.4563, 0.4590, 0.4654],\n",
       "          [0.4685, 0.4566, 0.4590,  ..., 0.4581, 0.4584, 0.4663],\n",
       "          [0.4651, 0.4524, 0.4543,  ..., 0.4558, 0.4564, 0.4642],\n",
       "          ...,\n",
       "          [0.4651, 0.4510, 0.4507,  ..., 0.4526, 0.4509, 0.4601],\n",
       "          [0.4648, 0.4525, 0.4509,  ..., 0.4526, 0.4519, 0.4590],\n",
       "          [0.4661, 0.4606, 0.4603,  ..., 0.4643, 0.4618, 0.4674]],\n",
       "\n",
       "         [[0.4254, 0.4285, 0.4286,  ..., 0.4212, 0.4160, 0.4091],\n",
       "          [0.4235, 0.4267, 0.4250,  ..., 0.4166, 0.4124, 0.4063],\n",
       "          [0.4241, 0.4268, 0.4267,  ..., 0.4179, 0.4128, 0.4070],\n",
       "          ...,\n",
       "          [0.4212, 0.4249, 0.4273,  ..., 0.4175, 0.4120, 0.4095],\n",
       "          [0.4184, 0.4227, 0.4232,  ..., 0.4157, 0.4126, 0.4090],\n",
       "          [0.4142, 0.4152, 0.4148,  ..., 0.4082, 0.4054, 0.4070]],\n",
       "\n",
       "         [[0.4303, 0.4264, 0.4271,  ..., 0.4268, 0.4281, 0.4268],\n",
       "          [0.4283, 0.4242, 0.4254,  ..., 0.4254, 0.4262, 0.4270],\n",
       "          [0.4261, 0.4222, 0.4230,  ..., 0.4245, 0.4248, 0.4253],\n",
       "          ...,\n",
       "          [0.4263, 0.4214, 0.4211,  ..., 0.4218, 0.4221, 0.4233],\n",
       "          [0.4268, 0.4220, 0.4213,  ..., 0.4230, 0.4224, 0.4221],\n",
       "          [0.4250, 0.4196, 0.4193,  ..., 0.4219, 0.4203, 0.4221]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.4989, 0.5024, 0.5024,  ..., 0.5037, 0.5048, 0.5062],\n",
       "          [0.5001, 0.5048, 0.5051,  ..., 0.5072, 0.5084, 0.5082],\n",
       "          [0.5010, 0.5065, 0.5057,  ..., 0.5071, 0.5075, 0.5082],\n",
       "          ...,\n",
       "          [0.5017, 0.5069, 0.5048,  ..., 0.5074, 0.5111, 0.5074],\n",
       "          [0.5032, 0.5056, 0.5068,  ..., 0.5086, 0.5078, 0.5085],\n",
       "          [0.5061, 0.5080, 0.5081,  ..., 0.5091, 0.5102, 0.5073]],\n",
       "\n",
       "         [[0.4596, 0.4478, 0.4490,  ..., 0.4509, 0.4552, 0.4609],\n",
       "          [0.4599, 0.4462, 0.4491,  ..., 0.4514, 0.4536, 0.4618],\n",
       "          [0.4559, 0.4419, 0.4439,  ..., 0.4490, 0.4517, 0.4591],\n",
       "          ...,\n",
       "          [0.4571, 0.4413, 0.4406,  ..., 0.4456, 0.4458, 0.4547],\n",
       "          [0.4579, 0.4441, 0.4419,  ..., 0.4465, 0.4476, 0.4529],\n",
       "          [0.4580, 0.4496, 0.4494,  ..., 0.4560, 0.4543, 0.4596]],\n",
       "\n",
       "         [[0.4209, 0.4247, 0.4243,  ..., 0.4247, 0.4241, 0.4232],\n",
       "          [0.4217, 0.4258, 0.4250,  ..., 0.4252, 0.4247, 0.4234],\n",
       "          [0.4227, 0.4263, 0.4258,  ..., 0.4257, 0.4258, 0.4245],\n",
       "          ...,\n",
       "          [0.4226, 0.4268, 0.4276,  ..., 0.4271, 0.4262, 0.4256],\n",
       "          [0.4223, 0.4270, 0.4273,  ..., 0.4265, 0.4271, 0.4263],\n",
       "          [0.4231, 0.4269, 0.4271,  ..., 0.4258, 0.4267, 0.4247]]]],\n",
       "       grad_fn=<SigmoidBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 313
    }
   ],
   "source": [
    "model = attnNet(IN_CHANNELS, H, W, batch_size)\n",
    "\n",
    "model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([2, 32, 224, 224])"
      ]
     },
     "metadata": {},
     "execution_count": 314
    }
   ],
   "source": [
    "model(input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}